{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "96c8d51e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      " train  set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 10000/10000 [00:00<00:00, 1109281.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge-1\n",
      "\tPrecision:\t 0.2714084271091003\n",
      "\tRecall:\t\t 0.35620294032443606\n",
      "\tF1:\t\t 0.3080775294444611\n",
      "rouge-2\n",
      "\tPrecision:\t 0.10551382384766454\n",
      "\tRecall:\t\t 0.14038935860727692\n",
      "\tF1:\t\t 0.12047845746680498\n",
      "rouge-4\n",
      "\tPrecision:\t 0.03930735395248246\n",
      "\tRecall:\t\t 0.052242200107432885\n",
      "\tF1:\t\t 0.04486100826958371\n",
      "rouge-l\n",
      "\tPrecision:\t 0.25137779860101084\n",
      "\tRecall:\t\t 0.3297205458133045\n",
      "\tF1:\t\t 0.28526815041474884\n",
      "\n",
      "\n",
      "\n",
      " validation  set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 1049100.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge-1\n",
      "\tPrecision:\t 0.3136344490928959\n",
      "\tRecall:\t\t 0.38441259161845703\n",
      "\tF1:\t\t 0.345435262568448\n",
      "rouge-2\n",
      "\tPrecision:\t 0.12817218618728507\n",
      "\tRecall:\t\t 0.15930864328114958\n",
      "\tF1:\t\t 0.1420542519348566\n",
      "rouge-4\n",
      "\tPrecision:\t 0.04829823623763358\n",
      "\tRecall:\t\t 0.059919139467185495\n",
      "\tF1:\t\t 0.05348473356137698\n",
      "rouge-l\n",
      "\tPrecision:\t 0.2895356076879822\n",
      "\tRecall:\t\t 0.35504441637642126\n",
      "\tF1:\t\t 0.3189611747617577\n",
      "\n",
      "\n",
      "\n",
      " test  set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 1006552.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge-1\n",
      "\tPrecision:\t 0.3051640128265358\n",
      "\tRecall:\t\t 0.3895018900973398\n",
      "\tF1:\t\t 0.34221331228531576\n",
      "rouge-2\n",
      "\tPrecision:\t 0.12652766723667142\n",
      "\tRecall:\t\t 0.16089493083217069\n",
      "\tF1:\t\t 0.14165664359852587\n",
      "rouge-4\n",
      "\tPrecision:\t 0.04644833653052124\n",
      "\tRecall:\t\t 0.05665898250357198\n",
      "\tF1:\t\t 0.05104808293837277\n",
      "rouge-l\n",
      "\tPrecision:\t 0.28223028618179574\n",
      "\tRecall:\t\t 0.36002475342139845\n",
      "\tF1:\t\t 0.316416012098339\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from evaluation.rouge_evaluator import RougeEvaluator\n",
    "import json\n",
    "import tqdm\n",
    "\n",
    "evaluator = RougeEvaluator()\n",
    "\n",
    "for data_type in [\"train\", \"validation\", \"test\"]:\n",
    "    print(\"\\n\\n\\n\", data_type, \" set\")\n",
    "    with open(f\"./data/{data_type}.json\", 'r') as f:\n",
    "        eval_data = json.load(f)\n",
    "\n",
    "    with open(f\"./models/{data_type}_pred_data.json\", 'r') as f:\n",
    "        pred_data = json.load(f)\n",
    "\n",
    "    assert len(eval_data) == len(pred_data)\n",
    "\n",
    "    pred_sums = []\n",
    "    eval_sums = []\n",
    "    for i, (eval, pred) in enumerate(tqdm.tqdm(zip(eval_data, pred_data), total=len(eval_data))):\n",
    "        pred_sums.append(pred['summary'])\n",
    "        eval_sums.append(eval['summary'])\n",
    "\n",
    "    scores = evaluator.batch_score(pred_sums, eval_sums)\n",
    "\n",
    "    for k, v in scores.items():\n",
    "        print(k)\n",
    "        print(\"\\tPrecision:\\t\", v[\"p\"])\n",
    "        print(\"\\tRecall:\\t\\t\", v[\"r\"])\n",
    "        print(\"\\tF1:\\t\\t\", v[\"f\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5714259",
   "metadata": {},
   "source": [
    "# Baseline 1 (three random sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0a316db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 982963.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge-1\n",
      "\tPrecision:\t 0.23100286990632624\n",
      "\tRecall:\t\t 0.24776795743078206\n",
      "\tF1:\t\t 0.23909188266827794\n",
      "rouge-2\n",
      "\tPrecision:\t 0.06332632996844961\n",
      "\tRecall:\t\t 0.06857402872031029\n",
      "\tF1:\t\t 0.06584578864194349\n",
      "rouge-4\n",
      "\tPrecision:\t 0.019636750129786787\n",
      "\tRecall:\t\t 0.02076334848861271\n",
      "\tF1:\t\t 0.020184341131428977\n",
      "rouge-l\n",
      "\tPrecision:\t 0.21054500801958181\n",
      "\tRecall:\t\t 0.22532294442134868\n",
      "\tF1:\t\t 0.21768345607661016\n"
     ]
    }
   ],
   "source": [
    "with open(\"./data/test.json\", 'r') as f:\n",
    "    eval_data = json.load(f)\n",
    "    \n",
    "with open(\"./models/test_data_baseline.json\", 'r') as f:\n",
    "    pred_data = json.load(f)\n",
    "\n",
    "assert len(eval_data) == len(pred_data)\n",
    "\n",
    "pred_sums = []\n",
    "eval_sums = []\n",
    "for eval, pred in tqdm.tqdm(zip(eval_data, pred_data), total=len(eval_data)):\n",
    "    pred_sums.append(pred['summary'])\n",
    "    eval_sums.append(eval['summary'])\n",
    "\n",
    "scores = evaluator.batch_score(pred_sums, eval_sums)\n",
    "\n",
    "for k, v in scores.items():\n",
    "    print(k)\n",
    "    print(\"\\tPrecision:\\t\", v[\"p\"])\n",
    "    print(\"\\tRecall:\\t\\t\", v[\"r\"])\n",
    "    print(\"\\tF1:\\t\\t\", v[\"f\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77a9a35",
   "metadata": {},
   "source": [
    "# Baseline 2 (greedy search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3b716d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 10000/10000 [00:00<00:00, 1803226.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge-1\n",
      "\tPrecision:\t 0.4785132510398555\n",
      "\tRecall:\t\t 0.469473039810747\n",
      "\tF1:\t\t 0.4739500406779775\n",
      "rouge-2\n",
      "\tPrecision:\t 0.2683624508316805\n",
      "\tRecall:\t\t 0.2604720586466614\n",
      "\tF1:\t\t 0.26435839106091474\n",
      "rouge-4\n",
      "\tPrecision:\t 0.12957751651443403\n",
      "\tRecall:\t\t 0.12238090188768092\n",
      "\tF1:\t\t 0.12587643180148803\n",
      "rouge-l\n",
      "\tPrecision:\t 0.45812417387875837\n",
      "\tRecall:\t\t 0.44864043109582874\n",
      "\tF1:\t\t 0.4533327078203429\n"
     ]
    }
   ],
   "source": [
    "with open(\"./data/train.json\", 'r') as f:\n",
    "    eval_data = json.load(f)\n",
    "    \n",
    "with open(\"./models/train_greedy_pred_data.json\", 'r') as f:\n",
    "    pred_data = json.load(f)\n",
    "\n",
    "assert len(eval_data) == len(pred_data)\n",
    "\n",
    "pred_sums = []\n",
    "eval_sums = []\n",
    "for eval, pred in tqdm.tqdm(zip(eval_data, pred_data), total=len(eval_data)):\n",
    "    pred_sums.append(pred['summary'])\n",
    "    eval_sums.append(eval['summary'])\n",
    "\n",
    "scores = evaluator.batch_score(pred_sums, eval_sums)\n",
    "\n",
    "for k, v in scores.items():\n",
    "    print(k)\n",
    "    print(\"\\tPrecision:\\t\", v[\"p\"])\n",
    "    print(\"\\tRecall:\\t\\t\", v[\"r\"])\n",
    "    print(\"\\tF1:\\t\\t\", v[\"f\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
