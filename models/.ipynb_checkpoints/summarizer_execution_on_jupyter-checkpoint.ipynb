{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "afc18c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import json\n",
    "import random\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.chunk import ne_chunk\n",
    "from collections import Counter\n",
    "from nltk.tree import Tree\n",
    "from collections import defaultdict\n",
    "from nltk.parse import CoreNLPParser\n",
    "from nltk.parse.corenlp import CoreNLPDependencyParser\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import gensim.downloader as api\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "import math \n",
    "from rouge_metric import PyRouge\n",
    "import tqdm\n",
    "    \n",
    "class RougeEvaluator:\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.rouge = PyRouge(rouge_n=(1, 2, 4), rouge_l=True, rouge_w=False, rouge_s=False, rouge_su=False)\n",
    "\n",
    "    def batch_score(self, gen_summaries, reference_summaries):\n",
    "        score = self.rouge.evaluate(gen_summaries, [[x] for x in reference_summaries])\n",
    "        return score\n",
    "    \n",
    "    def score(self, gen_summary, reference_summary):\n",
    "        score = self.rouge.evaluate([gen_summary], [[reference_summary]])\n",
    "        return score\n",
    "\n",
    "def evaluating_validation_set():\n",
    "    evaluator = RougeEvaluator()\n",
    "    \n",
    "    with open(f\"../data/validation.json\", 'r') as f:\n",
    "        eval_data = json.load(f)\n",
    "\n",
    "    with open(\"val_preds.json\", 'r') as f:\n",
    "        pred_data = json.load(f)\n",
    "\n",
    "    assert len(eval_data) == len(pred_data)\n",
    "\n",
    "    pred_sums = []\n",
    "    eval_sums = []\n",
    "    for eval, pred in tqdm.tqdm(zip(eval_data, pred_data), total=len(eval_data)):\n",
    "        pred_sums.append(pred['summary'])\n",
    "        eval_sums.append(eval['summary'])\n",
    "\n",
    "    scores = evaluator.batch_score(pred_sums, eval_sums)\n",
    "    return scores['rouge-1'][\"f\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "771be52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec model\n",
    "model = api.load(\"word2vec-google-news-300\")\n",
    "\n",
    "def preprocess(X):\n",
    "    \"\"\"\n",
    "    X: list of list of sentences (i.e., comprising an article)\n",
    "    \"\"\"\n",
    "    split_articles = [[s.strip() for s in x.split('.')] for i, x in enumerate(X)]\n",
    "    return split_articles\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def shuffle_dataframe(df):\n",
    "    # Shuffle the DataFrame\n",
    "    df_shuffled = df.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    # Split the DataFrame back into X and y\n",
    "    X_shuffled = df_shuffled.drop('target', axis=1)\n",
    "    y_shuffled = df_shuffled['target']\n",
    "    \n",
    "    return X_shuffled, y_shuffled\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### See feature_engineering for the generation of data\n",
    "data_train = pd.read_csv(\"train_processed.csv\")\n",
    "# X_train, y_train = train_data.iloc[:,:-1], train_data.iloc[:,-1]\n",
    "X_val = pd.read_csv(\"validation_processed.csv\")\n",
    "X_test= pd.read_csv(\"test_processed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "45c52abc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_0</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_191.1</th>\n",
       "      <th>feature_192.1</th>\n",
       "      <th>feature_193.1</th>\n",
       "      <th>feature_194.1</th>\n",
       "      <th>feature_195.1</th>\n",
       "      <th>feature_196.1</th>\n",
       "      <th>feature_197.1</th>\n",
       "      <th>feature_198.1</th>\n",
       "      <th>feature_199.1</th>\n",
       "      <th>feature_200.1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35910</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35911</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35912</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35913</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35914</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>35915 rows × 639 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       feature_0  feature_1  feature_2  feature_3  feature_4  feature_5  \\\n",
       "0            1.0        0.0        0.0        1.0        1.0        0.0   \n",
       "1            0.0        0.0        0.0        1.0        3.0        0.0   \n",
       "2            1.0        0.0        0.0        0.0        1.0        0.0   \n",
       "3            1.0        0.0        0.0        0.0        1.0        0.0   \n",
       "4            0.0        0.0        0.0        0.0        1.0        0.0   \n",
       "...          ...        ...        ...        ...        ...        ...   \n",
       "35910        1.0        0.0        0.0        0.0        0.0        0.0   \n",
       "35911        0.0        0.0        0.0        1.0        0.0        0.0   \n",
       "35912        2.0        0.0        0.0        0.0        1.0        0.0   \n",
       "35913        0.0        0.0        0.0        0.0        0.0        0.0   \n",
       "35914        0.0        0.0        0.0        0.0        0.0        0.0   \n",
       "\n",
       "       feature_6  feature_7  feature_8  feature_9  ...  feature_191.1  \\\n",
       "0            0.0        0.0        0.0        0.0  ...              0   \n",
       "1            0.0        0.0        0.0        0.0  ...              0   \n",
       "2            0.0        0.0        0.0        0.0  ...              0   \n",
       "3            0.0        0.0        0.0        0.0  ...              0   \n",
       "4            0.0        0.0        0.0        0.0  ...              0   \n",
       "...          ...        ...        ...        ...  ...            ...   \n",
       "35910        0.0        0.0        0.0        0.0  ...              0   \n",
       "35911        0.0        0.0        0.0        0.0  ...              0   \n",
       "35912        0.0        0.0        0.0        0.0  ...              0   \n",
       "35913        0.0        0.0        0.0        0.0  ...              0   \n",
       "35914        0.0        0.0        0.0        0.0  ...              0   \n",
       "\n",
       "       feature_192.1  feature_193.1  feature_194.1  feature_195.1  \\\n",
       "0                  0              0              0              0   \n",
       "1                  0              0              0              0   \n",
       "2                  0              0              0              0   \n",
       "3                  0              0              0              0   \n",
       "4                  0              0              0              0   \n",
       "...              ...            ...            ...            ...   \n",
       "35910              0              0              0              0   \n",
       "35911              0              0              0              0   \n",
       "35912              0              0              0              0   \n",
       "35913              0              0              0              0   \n",
       "35914              0              0              0              0   \n",
       "\n",
       "       feature_196.1  feature_197.1  feature_198.1  feature_199.1  \\\n",
       "0                  0              0              0              0   \n",
       "1                  0              0              0              0   \n",
       "2                  0              0              0              0   \n",
       "3                  0              0              0              0   \n",
       "4                  0              0              0              0   \n",
       "...              ...            ...            ...            ...   \n",
       "35910              0              0              0              0   \n",
       "35911              0              0              0              0   \n",
       "35912              0              0              0              0   \n",
       "35913              0              0              0              0   \n",
       "35914              0              0              0              0   \n",
       "\n",
       "       feature_200.1  \n",
       "0                  0  \n",
       "1                  0  \n",
       "2                  0  \n",
       "3                  0  \n",
       "4                  0  \n",
       "...              ...  \n",
       "35910              0  \n",
       "35911              0  \n",
       "35912              0  \n",
       "35913              0  \n",
       "35914              0  \n",
       "\n",
       "[35915 rows x 639 columns]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "ec06bed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, hidden_dim1=64, hidden_dim2=64, output_dim=1):\n",
    "        self.W1 = self.b1 = self.W2 = self.b2 = self.W3 = self.b3 = None\n",
    "        self.hidden_dim1 = hidden_dim1\n",
    "        self.hidden_dim2 = hidden_dim2\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.mW1 = self.vW1 = self.mW2 = self.vW2 = self.mW3 = self.vW3 = None\n",
    "        self.mb1 = self.vb1 = self.mb2 = self.vb2 = self.mb3 = self.vb3 = None\n",
    "\n",
    "    def forward(self, X):\n",
    "        if self.W1 is None:\n",
    "            self.W1 = np.random.randn(X.shape[1], self.hidden_dim1) / np.sqrt(X.shape[1])\n",
    "            self.b1 = np.zeros((self.hidden_dim1, 1))\n",
    "            self.W2 = np.random.randn(self.hidden_dim1, self.hidden_dim2) / np.sqrt(self.hidden_dim1)\n",
    "            self.b2 = np.zeros((self.hidden_dim2, 1))\n",
    "            self.W3 = np.random.randn(self.hidden_dim2, self.output_dim) / np.sqrt(self.hidden_dim2)\n",
    "            self.b3 = np.zeros((self.output_dim, 1))\n",
    "\n",
    "            self.mW1 = np.zeros_like(self.W1)\n",
    "            self.vW1 = np.zeros_like(self.W1)\n",
    "            self.mb1 = np.zeros_like(self.b1)\n",
    "            self.vb1 = np.zeros_like(self.b1)\n",
    "\n",
    "            self.mW2 = np.zeros_like(self.W2)\n",
    "            self.vW2 = np.zeros_like(self.W2)\n",
    "            self.mb2 = np.zeros_like(self.b2)\n",
    "            self.vb2 = np.zeros_like(self.b2)\n",
    "\n",
    "            self.mW3 = np.zeros_like(self.W3)\n",
    "            self.vW3 = np.zeros_like(self.W3)\n",
    "            self.mb3 = np.zeros_like(self.b3)\n",
    "            self.vb3 = np.zeros_like(self.b3)\n",
    "\n",
    "        self.z1 = X.dot(self.W1) + self.b1.T\n",
    "        self.a1 = np.maximum(0, self.z1)\n",
    "        self.z2 = self.a1.dot(self.W2) + self.b2.T\n",
    "        self.a2 = np.maximum(0, self.z2)\n",
    "        self.z3 = self.a2.dot(self.W3) + self.b3.T\n",
    "        self.output = sigmoid(self.z3)\n",
    "        return self.output\n",
    "\n",
    "    def train(self, data_train, X_val=None, epochs=20, initial_lr=0.001, batch_size=64, beta1=0.9, beta2=0.9, epsilon=1e-8, patience=3, lambda_l2=0.001):\n",
    "        learning_rate = initial_lr\n",
    "        best_val_loss = float('inf')\n",
    "        epochs_without_improvement = 0\n",
    "        losses = []\n",
    "        val_losses = [0]\n",
    "\n",
    "        t = 0\n",
    "        for epoch in range(epochs):\n",
    "            X_train, y_train = shuffle_dataframe(data_train)\n",
    "            X_shuffled, y_shuffled = X_train, y_train\n",
    "            loss_counter = 0\n",
    "            for i in range(0, len(data_train)//64):\n",
    "                t += 1\n",
    "                \"\"\"\n",
    "                IMPORTANT: it's considered that batch_size == #sentences\n",
    "                \"\"\"\n",
    "                try:\n",
    "                    X_batch = X_shuffled.iloc[i*64:i*64+64]  \n",
    "                    y_batch = y_shuffled.iloc[i*64:i*64+64]\n",
    "\n",
    "                except:\n",
    "                    X_batch = X_shuffled.iloc[i*64:]  \n",
    "                    y_batch = y_shuffled.iloc[i*64:]\n",
    "          \n",
    "\n",
    "                \"\"\"\n",
    "                NOW: Do feature engineering for each article's sentences (batch) and then do forward & backprop\n",
    "                \"\"\" \n",
    "                X_batch = np.array(X_batch)\n",
    "                y_batch = np.array(y_batch).reshape(-1, 1)  # Reshape to make it a column vector\n",
    "    \n",
    "                output = self.forward(X_batch)    \n",
    "                error = output - y_batch\n",
    "                loss_counter += np.mean(np.square(error))\n",
    "                # Weight the error by instance\n",
    "                instance_weights = np.where(y_batch == 1, 17, 1) # Determine instance weights based on class labels\n",
    "                weighted_error = error * instance_weights\n",
    "    \n",
    "                # Adjust the loss computation to use the weighted error\n",
    "                mse_loss = np.mean(np.square(weighted_error))\n",
    "                l2_loss = lambda_l2 * (np.sum(np.square(self.W1)) + np.sum(np.square(self.W2)) + np.sum(np.square(self.W3)))\n",
    "                total_loss = mse_loss + l2_loss\n",
    "\n",
    "                if i%2000==0:\n",
    "                    print(\"epoch=\", epoch, \"loss=\",loss_counter/i)\n",
    "\n",
    "                losses.append(total_loss)\n",
    "\n",
    "                # Adjust backpropagation to use the weighted error\n",
    "                sigmoid_derivative = output * (1 - output)\n",
    "                weighted_error *= sigmoid_derivative\n",
    "                \n",
    "                # Now continue with backpropagation as usual, but use 'weighted_error' instead of 'error'\n",
    "                dW3 = (self.a2.T).dot(2 * weighted_error)\n",
    "                db3 = np.sum(2 * weighted_error, axis=0, keepdims=True).T\n",
    "                da2 = (2 * weighted_error).dot(self.W3.T)\n",
    "                dz2 = da2 * (self.a2 > 0)\n",
    "                dW2 = (self.a1.T).dot(dz2)\n",
    "                db2 = np.sum(dz2, axis=0, keepdims=True).T\n",
    "                da1 = dz2.dot(self.W2.T)\n",
    "                dz1 = da1 * (self.a1 > 0)\n",
    "                dW1 = np.dot(X_batch.T, dz1)\n",
    "                db1 = np.sum(dz1, axis=0, keepdims=True).T\n",
    "\n",
    "                # Adding the regularisation term to the gradients\n",
    "                dW3 += 2 * lambda_l2 * self.W3\n",
    "                dW2 += 2 * lambda_l2 * self.W2\n",
    "                dW1 += 2 * lambda_l2 * self.W1\n",
    "\n",
    "                self.mW1 = beta1 * self.mW1 + (1 - beta1) * dW1\n",
    "                self.vW1 = beta2 * self.vW1 + (1 - beta2) * np.square(dW1)\n",
    "                mW1_corr = self.mW1 / (1 - beta1 ** t)\n",
    "                vW1_corr = self.vW1 / (1 - beta2 ** t)\n",
    "\n",
    "                self.mW2 = beta1 * self.mW2 + (1 - beta1) * dW2\n",
    "                self.vW2 = beta2 * self.vW2 + (1 - beta2) * np.square(dW2)\n",
    "                mW2_corr = self.mW2 / (1 - beta1 ** t)\n",
    "                vW2_corr = self.vW2 / (1 - beta2 ** t)\n",
    "\n",
    "                self.mW3 = beta1 * self.mW3 + (1 - beta1) * dW3\n",
    "                self.vW3 = beta2 * self.vW3 + (1 - beta2) * np.square(dW3)\n",
    "                mW3_corr = self.mW3 / (1 - beta1 ** t)\n",
    "                vW3_corr = self.vW3 / (1 - beta2 ** t)\n",
    "\n",
    "                self.mb1 = beta1 * self.mb1 + (1 - beta1) * db1\n",
    "                self.vb1 = beta2 * self.vb1 + (1 - beta2) * np.square(db1)\n",
    "                mb1_corr = self.mb1 / (1 - beta1 ** t)\n",
    "                vb1_corr = self.vb1 / (1 - beta2 ** t)\n",
    "\n",
    "                self.mb2 = beta1 * self.mb2 + (1 - beta1) * db2\n",
    "                self.vb2 = beta2 * self.vb2 + (1 - beta2) * np.square(db2)\n",
    "                mb2_corr = self.mb2 / (1 - beta1 ** t)\n",
    "                vb2_corr = self.vb2 / (1 - beta2 ** t)\n",
    "\n",
    "                self.mb3 = beta1 * self.mb3 + (1 - beta1) * db3\n",
    "                self.vb3 = beta2 * self.vb3 + (1 - beta2) * np.square(db3)\n",
    "                mb3_corr = self.mb3 / (1 - beta1 ** t)\n",
    "                vb3_corr = self.vb3 / (1 - beta2 ** t)\n",
    "\n",
    "                self.W1 -= learning_rate * (mW1_corr / (np.sqrt(vW1_corr) + epsilon) + 2 * lambda_l2 * self.W1)\n",
    "                self.W2 -= learning_rate * (mW2_corr / (np.sqrt(vW2_corr) + epsilon) + 2 * lambda_l2 * self.W2)\n",
    "                self.W3 -= learning_rate * (mW3_corr / (np.sqrt(vW3_corr) + epsilon) + 2 * lambda_l2 * self.W3)\n",
    "\n",
    "                self.b1 -= learning_rate * (mb1_corr / (np.sqrt(vb1_corr) + epsilon))\n",
    "                self.b2 -= learning_rate * (mb2_corr / (np.sqrt(vb2_corr) + epsilon))\n",
    "                self.b3 -= learning_rate * (mb3_corr / (np.sqrt(vb3_corr) + epsilon))\n",
    "                \n",
    "\n",
    "            \"\"\"\n",
    "            REMEMBER TO CHANGE/COMPLETE THIS to the real VAL set FOR THE FINAL VERSION\n",
    "            \"\"\"  \n",
    "            if True:\n",
    "                X_val_batch = X_val\n",
    "                X_val_batch = np.array(X_val_batch)\n",
    "                val_preds = self.forward(X_val_batch)\n",
    "\n",
    "                with open(\"../data/validation.json\", 'r') as f:\n",
    "                    eval_data = json.load(f)\n",
    "\n",
    "                eval_articles = [article['article'] for article in eval_data]\n",
    "                preprocessed_val_articles = [[s.strip() for s in x.split('.')] for i, x in enumerate(eval_articles)]\n",
    "\n",
    "                summaries = summary_extraction(preprocessed_val_articles, val_preds)\n",
    "                pred_data = [{'article': article, 'summary': summary} for article, summary in zip(eval_articles, summaries)]\n",
    "\n",
    "                with open(\"val_preds.json\", 'w') as f:\n",
    "                    json.dump(pred_data, f, indent=4)  # indent parameter is optional, it makes the output more readable\n",
    "    \n",
    "                val_rogue_f1 = evaluating_validation_set()\n",
    "                best_val_rogue_f1 = max(val_losses)\n",
    "                \n",
    "                if val_rogue_f1 > best_val_rogue_f1:\n",
    "                    best_val_rogue_f1 = val_rogue_f1\n",
    "                    epochs_without_improvement = 0\n",
    "                else:\n",
    "                    epochs_without_improvement += 1\n",
    "                    print(\"epochs num: \", epochs_without_improvement,\n",
    "                          \"\\nval_rogue_f1: \", val_rogue_f1)\n",
    "                    learning_rate *= 0.9\n",
    "                    \n",
    "                val_losses.append(val_rogue_f1)\n",
    "\n",
    "                print(f'\\nEpoch {epoch+1}/{epochs}\\n')\n",
    "                print(f'Val ROGUE-1 F1: {val_rogue_f1:.3f}, Learning Rate: {learning_rate:.6f}\\n')\n",
    "\n",
    "                if epochs_without_improvement >= patience:\n",
    "                    print(\"Early stopping triggered.\")\n",
    "                    break\n",
    "                \n",
    "            \n",
    "    def test(self, X_test):\n",
    "        X_test_batch = X_test  \n",
    "\n",
    "        X_test_batch = np.array(X_test_batch)\n",
    "\n",
    "        test_output = self.forward(X_test_batch)\n",
    "\n",
    "        return test_output\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "c0fb0c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Center and scale the training data\n",
    "X_train = data_train.iloc[:,:-1]\n",
    "train_mean = np.mean(X_train, axis=0)\n",
    "train_std = np.std(X_train, axis=0)\n",
    "X_train_scaled = (X_train - train_mean) / train_std\n",
    "data_train_scaled = data_train\n",
    "data_train_scaled.iloc[:,:-1] = X_train_scaled\n",
    "data_train_scaled.fillna(0)\n",
    "\n",
    "# scale the validation and test data using the training mean and std:\n",
    "X_val_scaled = (X_val - train_mean) / train_std\n",
    "X_test_scaled = (X_test - train_mean) / train_std\n",
    "X_val_scaled.fillna(0)\n",
    "X_test_scaled.fillna(0)\n",
    "\n",
    "# Identify columns that contain NaN values in the training set and remove these columns on training, validation and test sets\n",
    "nan_columns = data_train_scaled.columns[data_train_scaled.isna().any()].tolist()\n",
    "data_train_scaled = data_train_scaled.drop(nan_columns, axis=1)\n",
    "X_val_scaled = X_val_scaled.drop(nan_columns, axis=1)\n",
    "X_test_scaled = X_test_scaled.drop(nan_columns, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "37a33b51",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\17245\\AppData\\Local\\Temp\\ipykernel_14208\\1233792759.py:88: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  print(\"epoch=\", epoch, \"loss=\",loss_counter/i)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 0 loss= inf\n",
      "epoch= 0 loss= 0.2171515148697838\n",
      "epoch= 0 loss= 0.21450771458020232\n",
      "epoch= 0 loss= 0.21342162233264034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/30\n",
      "\n",
      "Val ROGUE-1 F1: 0.343, Learning Rate: 0.001000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\17245\\AppData\\Local\\Temp\\ipykernel_14208\\1233792759.py:88: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  print(\"epoch=\", epoch, \"loss=\",loss_counter/i)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 1 loss= inf\n",
      "epoch= 1 loss= 0.21020990593301028\n",
      "epoch= 1 loss= 0.21005891486197165\n",
      "epoch= 1 loss= 0.20904689182793212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 998406.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/30\n",
      "\n",
      "Val ROGUE-1 F1: 0.345, Learning Rate: 0.001000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\17245\\AppData\\Local\\Temp\\ipykernel_14208\\1233792759.py:88: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  print(\"epoch=\", epoch, \"loss=\",loss_counter/i)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 2 loss= inf\n",
      "epoch= 2 loss= 0.20982777472249733\n",
      "epoch= 2 loss= 0.2099803824307542\n",
      "epoch= 2 loss= 0.20826197184953293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 982042.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs num:  1 \n",
      "val_rogue_f1:  0.3449009538204685\n",
      "\n",
      "Epoch 3/30\n",
      "\n",
      "Val ROGUE-1 F1: 0.345, Learning Rate: 0.000900\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\17245\\AppData\\Local\\Temp\\ipykernel_14208\\1233792759.py:88: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  print(\"epoch=\", epoch, \"loss=\",loss_counter/i)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 3 loss= inf\n",
      "epoch= 3 loss= 0.20720309611207738\n",
      "epoch= 3 loss= 0.20749972779823508\n",
      "epoch= 3 loss= 0.20794888954486965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 1000788.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4/30\n",
      "\n",
      "Val ROGUE-1 F1: 0.346, Learning Rate: 0.000900\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\17245\\AppData\\Local\\Temp\\ipykernel_14208\\1233792759.py:88: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  print(\"epoch=\", epoch, \"loss=\",loss_counter/i)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 4 loss= inf\n",
      "epoch= 4 loss= 0.20642852416550234\n",
      "epoch= 4 loss= 0.20793284167383305\n",
      "epoch= 4 loss= 0.20798112894631235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 1000549.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5/30\n",
      "\n",
      "Val ROGUE-1 F1: 0.348, Learning Rate: 0.000900\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\17245\\AppData\\Local\\Temp\\ipykernel_14208\\1233792759.py:88: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  print(\"epoch=\", epoch, \"loss=\",loss_counter/i)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 5 loss= inf\n",
      "epoch= 5 loss= 0.20812544467377256\n",
      "epoch= 5 loss= 0.20814373799287028\n",
      "epoch= 5 loss= 0.20742042324493817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs num:  1 \n",
      "val_rogue_f1:  0.34474699314217416\n",
      "\n",
      "Epoch 6/30\n",
      "\n",
      "Val ROGUE-1 F1: 0.345, Learning Rate: 0.000810\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\17245\\AppData\\Local\\Temp\\ipykernel_14208\\1233792759.py:88: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  print(\"epoch=\", epoch, \"loss=\",loss_counter/i)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 6 loss= inf\n",
      "epoch= 6 loss= 0.205960531422735\n",
      "epoch= 6 loss= 0.20761906661350446\n",
      "epoch= 6 loss= 0.2072713690250376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 1001266.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs num:  2 \n",
      "val_rogue_f1:  0.3468641153209727\n",
      "\n",
      "Epoch 7/30\n",
      "\n",
      "Val ROGUE-1 F1: 0.347, Learning Rate: 0.000729\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\17245\\AppData\\Local\\Temp\\ipykernel_14208\\1233792759.py:88: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  print(\"epoch=\", epoch, \"loss=\",loss_counter/i)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch= 7 loss= inf\n",
      "epoch= 7 loss= 0.20372580244856564\n",
      "epoch= 7 loss= 0.20549382068355243\n",
      "epoch= 7 loss= 0.20832705985340252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 499797.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs num:  3 \n",
      "val_rogue_f1:  0.34588436379011667\n",
      "\n",
      "Epoch 8/30\n",
      "\n",
      "Val ROGUE-1 F1: 0.346, Learning Rate: 0.000656\n",
      "\n",
      "Early stopping triggered.\n"
     ]
    }
   ],
   "source": [
    "# Initialize and train MLP\n",
    "mlp = MLP()\n",
    "mlp.train(data_train_scaled, X_val=X_val_scaled, epochs=30, initial_lr=0.001, \n",
    "          batch_size=64, beta1=0.9, beta2=0.9, epsilon=1e-8, lambda_l2=0.001)\n",
    "\n",
    "# Evaluate the model on test set\n",
    "# test_output = mlp.forward(feature_engineering(X_test))\n",
    "# test_predictions = (test_output >= 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "b2747600",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds_greedy = data_train[\"target\"]\n",
    "train_preds = mlp.test(data_train_scaled.iloc[:,:-1])\n",
    "val_preds = mlp.test(X_val_scaled)\n",
    "test_preds = mlp.test(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "7407507a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "\n",
    "def top_m_indicator(a_preds, m):\n",
    "    # Step 1: Identify the top m highest probabilities and their indices\n",
    "    a_preds = [float(p) for p in a_preds]\n",
    "\n",
    "    top_m_indices = np.argsort(a_preds)[-m:]\n",
    "    \n",
    "    # Step 2: Create a new list of zeros\n",
    "    indicator_list = np.zeros(len(a_preds), dtype=int)\n",
    "    \n",
    "    # Step 3: Set the elements corresponding to the top m probabilities to 1\n",
    "    indicator_list[top_m_indices] = 1\n",
    "    \n",
    "    return indicator_list.tolist()\n",
    "\n",
    "\n",
    "def summary_extraction(prepro_articles, preds):\n",
    "    i = 0\n",
    "    summaries = []\n",
    "    for a in prepro_articles:\n",
    "        a_preds = preds[i:i+len(a)]\n",
    "        m = 3\n",
    "        summary_index = top_m_indicator(a_preds, m)\n",
    "        summary = [s for i, s in enumerate(a) if summary_index[i] == 1]\n",
    "        summary = '\\n'.join(summary)\n",
    "        summaries.append(summary)\n",
    "        i += len(a)\n",
    "        \n",
    "    return summaries\n",
    "\n",
    "with open(\"../data/validation.json\", 'r') as f:\n",
    "    eval_data = json.load(f)\n",
    "\n",
    "eval_articles = [article['article'] for article in eval_data]\n",
    "preprocessed_val_articles = preprocess(eval_articles) \n",
    "\n",
    "summaries = summary_extraction(preprocessed_val_articles, val_preds)\n",
    "pred_data = [{'article': article, 'summary': summary} for article, summary in zip(eval_articles, summaries)]\n",
    "\n",
    "with open(\"validation_pred_data.json\", 'w') as f:\n",
    "    json.dump(pred_data, f, indent=4)  # indent parameter is optional, it makes the output more readable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "1fa01213",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/test.json\", 'r') as f:\n",
    "    eval_data = json.load(f)\n",
    "\n",
    "eval_articles = [article['article'] for article in eval_data]\n",
    "preprocessed_val_articles = preprocess(eval_articles) \n",
    "\n",
    "summaries = summary_extraction(preprocessed_val_articles, test_preds)\n",
    "pred_data = [{'article': article, 'summary': summary} for article, summary in zip(eval_articles, summaries)]\n",
    "\n",
    "with open(\"test_pred_data.json\", 'w') as f:\n",
    "    json.dump(pred_data, f, indent=4)  # indent parameter is optional, it makes the output more readable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "d4a764cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/train.json\", 'r') as f:\n",
    "    eval_data = json.load(f)\n",
    "\n",
    "eval_articles = [article['article'] for article in eval_data]\n",
    "preprocessed_val_articles = preprocess(eval_articles) \n",
    "\n",
    "summaries = summary_extraction(preprocessed_val_articles, train_preds)\n",
    "pred_data = [{'article': article, 'summary': summary} for article, summary in zip(eval_articles, summaries)]\n",
    "\n",
    "with open(\"train_pred_data.json\", 'w') as f:\n",
    "    json.dump(pred_data, f, indent=4)  # indent parameter is optional, it makes the output more readable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "df1328c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Best ROGUE scores that we can achieve\n",
    "\"\"\"\n",
    "\n",
    "def summary_extraction_train(prepro_articles, preds):\n",
    "    i = 0\n",
    "    summaries = []\n",
    "    for a in prepro_articles:\n",
    "        a_preds = preds[i:i+len(a)]\n",
    "        summary = [s for i, s in enumerate(a) if a_preds.iloc[i] == 1]\n",
    "        summary = '\\n'.join(summary)\n",
    "        summaries.append(summary)\n",
    "        i += len(a)\n",
    "        \n",
    "    return summaries\n",
    "\n",
    "with open(\"../data/train.json\", 'r') as f:\n",
    "    eval_data = json.load(f)\n",
    "\n",
    "eval_articles = [article['article'] for article in eval_data]\n",
    "preprocessed_val_articles = preprocess(eval_articles) \n",
    "\n",
    "summaries = summary_extraction_train(preprocessed_val_articles, train_preds_greedy)\n",
    "pred_data = [{'article': article, 'summary': summary} for article, summary in zip(eval_articles, summaries)]\n",
    "\n",
    "with open(\"train_greedy_pred_data.json\", 'w') as f:\n",
    "    json.dump(pred_data, f, indent=4)  # indent parameter is optional, it makes the output more readable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3be953",
   "metadata": {},
   "source": [
    "# Compute the baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "865f0e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_summary_extraction(prepro_articles, preds):\n",
    "    i = 0\n",
    "    summaries = []\n",
    "    for a in prepro_articles:\n",
    "        n = len(a)\n",
    "        a_preds = preds[i:i+n]\n",
    "        m = 3\n",
    "        \n",
    "        ones_list = [1] * m\n",
    "        zeros_list = [0] * (n-m)\n",
    "        summary_index = ones_list + zeros_list\n",
    "        random.shuffle(summary_index)\n",
    "        \n",
    "        summary = [s for i, s in enumerate(a) if summary_index[i] == 1]\n",
    "        summary = '\\n'.join(summary)\n",
    "        summaries.append(summary)\n",
    "        i += len(a)\n",
    "        \n",
    "    return summaries\n",
    "\n",
    "with open(\"C:/Users/17245/MPhil ACS/L90-Summarization-main/data/validation.json\", 'r') as f:\n",
    "    eval_data = json.load(f)\n",
    "\n",
    "eval_articles = [article['article'] for article in eval_data]\n",
    "preprocessed_val_articles = preprocess(eval_articles) \n",
    "\n",
    "summaries = random_summary_extraction(preprocessed_val_articles, val_preds)\n",
    "pred_data = [{'article': article, 'summary': summary} for article, summary in zip(eval_articles, summaries)]\n",
    "\n",
    "with open(\"val_data_baseline.json\", 'w') as f:\n",
    "    json.dump(pred_data, f, indent=4)  # indent parameter is optional, it makes the output more readable\n",
    "    \n",
    "    \n",
    "with open(\"C:/Users/17245/MPhil ACS/L90-Summarization-main/data/test.json\", 'r') as f:\n",
    "    eval_data = json.load(f)\n",
    "\n",
    "eval_articles = [article['article'] for article in eval_data]\n",
    "preprocessed_val_articles = preprocess(eval_articles) \n",
    "\n",
    "summaries = random_summary_extraction(preprocessed_val_articles, val_preds)\n",
    "pred_data = [{'article': article, 'summary': summary} for article, summary in zip(eval_articles, summaries)]\n",
    "    \n",
    "with open(\"test_data_baseline.json\", 'w') as f:\n",
    "    json.dump(pred_data, f, indent=4)  # indent parameter is optional, it makes the output more readable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a044329",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
